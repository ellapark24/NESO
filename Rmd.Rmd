---
title: "Modelling Average Electricity Demand in Great Britain"
author: "Ella Park (s2311400), Rosie Crookes (s2333530), Kieran Marguerie de Rotrou (s2536961)"
output:
  html_document:
    number_sections: no
  pdf_document:
    number_sections: no
header-includes:
  - \newcommand{\bm}[1]{\boldsymbol{#1}}
  - \newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
---
#### Word count: 1092
```{r setup, include=FALSE}
# Modify this setup code chunk to set options
# or add extra packages etc if needed.
# See the project instructions for more details
# on what code to show, and where/how.

# Set default code chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE
)

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(patchwork))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(gridExtra))

theme_set(theme_bw())
# To give the same random number sequence every time the document is knited,
# making it easier to discuss the specific numbers in the text:
set.seed(12345L)
```

```{r code=readLines("code.R"), eval=TRUE, echo=FALSE, results='hide', include=FALSE}
# Do not change this code chunk
# Load function definitions
source("code.R")
```

# Executive Summary

The aim of this study is to predict average daily electricity demand to enable NESO to ensure a reliable supply. Forecasting is performed using an ordinary least squares linear regression model. This is selected by identifying key temporal and weather based predictors as well as considering important metrics such as $R^2$ and AIC. The model is then evaluated for it’s practicality and predictive ability using cross validation. We ultimately conclude, the model predicts average demand accurately while remaining simple and practical.

# Data Analysis and Exploration

Prior to fitting models we examine the dataset used in our analysis. We consider the following predictor variables for our linear model: 

- $\texttt{wind_merra}$: Average capacity factor of wind generation
- $\texttt{solar_sarah}$: Average capacity factor of solar generation
- $\texttt{temp_merra1}$: Average temperature 
- $\texttt{wdayindex}$: Day of the week - included as a factor variable
- $\texttt{month}$: Month - included as a factor variable
- $\texttt{year}$: Year - included as a factor variable. However, this will be stripped out of the model by NESO, who will add their own year effect for future years.
- $\texttt{DSN}$: Number of days since November
- $\texttt{TO}$:  Average temperature from 3 PM – 6 PM on each day
- $\texttt{TE}$:  Average of $\texttt{TO}$ at 6 PM and $\texttt{TE}$ at 6 PM on the previous day ($\texttt{TE}$ for 1 November 1991 taken as 11.37 degrees celsius).

Figures and observations from our data exploration are recorded below.

```{r Exploratory Analysis, eval=TRUE, echo=FALSE, fig.height=30, fig.width=20, warning = 'false', message = 'false', results='hide'}

# Wind vs Demand
wind_plot <- ggplot(processed.data, aes(x=wind_merra1, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[1]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Average Demand vs Wind Capacity", x="Wind Capacity Factor", y="Average Demand (MW)",
       caption = "Figure 1: Wind generation showed no clear correlation with demand.") +
  theme(text = element_text(size = 16), plot.caption = element_text(size = 20, hjust = 0))  

# Solar vs Demand
solar_plot <- ggplot(processed.data, aes(x=solar_sarah, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[6]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Average Demand vs Solar Capacity", x="Solar Capacity Factor", y="Average Demand (MW)",
       caption="Figure 2: Solar generation exhibits an inverse relationship ") +
  theme(text = element_text(size = 16), plot.caption = element_text(size = 20, hjust = 0))

# Temp vs Demand- shows negative correlation
temp_plot <- ggplot(processed.data, aes(x=temp_merra1, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[8]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Average Demand vs Temperature", x="Temperature (°C)", y="Average Demand (MW)",
       caption="Figure 3: Shows a negative correlation with demand as expected.") +
  theme(text = element_text(size = 16), plot.caption = element_text(size = 20, hjust = 0)) 

# TE vs Demand - shows negative correlation
te_plot <- ggplot(processed.data, aes(x=TE, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[4]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Average Demand vs TE", x="TE (°C)", y="Average Demand (MW)", 
       caption="Figure 4: Shows a negative correlation with demand as expected.") +
  theme(text = element_text(size = 16), plot.caption = element_text(size = 20))  

# TO vs Demand
to_plot <- ggplot(processed.data, aes(x=TO, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[9]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Average Demand vs TO", x="TO (°C)", y="Average Demand (MW)",
       caption="Figure 5: Shows a negative correlation with demand as expected.") +
  theme(text = element_text(size = 16), plot.caption = element_text(size = 20, hjust = 0))  

# dsn vs Demand - shows negative correlation
dsn_plot <- ggplot(processed.data, aes(x=DSN, y=demand_gross)) +
  geom_point(alpha=0.5) +
  geom_smooth(method="lm", formula = y ~ poly(x,2), color="red") +
  labs(title="Average Demand vs DSN", x="Days since November", y="Average Demand (MW)",
       caption="Figure 6: Follows a quadratic distribution, with a dip in December.") +
  theme(text = element_text(size = 16), plot.caption = element_text(size = 20, hjust = 0))  

# Month vs demand
month_plot <- ggplot(processed.data, aes(x=factor(Month, levels = c(10, 11, 12, 1, 2)), y=demand_gross, fill=factor(Month))) +
  geom_violin(alpha=0.5) +  # Create the violin plot
  geom_boxplot(width=0.2, color="black", alpha=0.7) +  
  scale_fill_viridis_d(option="viridis") +    
  scale_x_discrete(labels=c("November", "December", "Janurary", "February", "March")) +
  labs(title="Average Demand Distribution by Month", x="Month", y="Average Demand (MW)", 
       caption = "Figure 7: Highest levels observed in January and lowest in November and March.") +
  theme_bw() +
  guides(fill=FALSE) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(text = element_text(size = 16), plot.caption = element_text(size = 20, hjust = 0))  

# Day of week
day_plot <- ggplot(processed.data, aes(x=factor(WeekdayNum), y=demand_gross, fill=factor(WeekdayNum))) +
  geom_violin(alpha=0.5) +  # Create the violin plot
  geom_boxplot(width=0.2, color="black", alpha=0.7) +  
  scale_fill_viridis_d(option="viridis") + 
  scale_x_discrete(labels=c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")) +
  labs(title="Average Demand by Day of the Week", x="Day of Week", y="Average Demand (MW)",
       caption="Figure 8: Lower demand on weekends and higher demand on weekdays.") +
  theme_bw() +
  guides(fill=FALSE) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  theme(text = element_text(size = 16), plot.caption = element_text(size = 20, hjust = 0))  

# Arrange in a grid
(wind_plot + solar_plot) / (temp_plot + to_plot) / (te_plot + dsn_plot) /  (month_plot + day_plot)   
  plot_annotation(title = "Exploratory Analysis of Demand Data", theme = theme(plot.title = element_text(size = 20, face = "bold")))+
  theme(text = element_text(size = 16))  
```

# Model Selection and Evaluation

We fit an ordinary least-squares linear regression model: $$\mathbf{y} = X \boldsymbol{\beta} + \boldsymbol{\epsilon}$$ assuming 

* Linearity of model parameters. 
* Errors, $\boldsymbol{\epsilon}$, are normally distributed. 
* Errors are independent.
* Errors have constant variance (homoscedasticity).

## Metrics

Variable significance is assessed through hypothesis testing model parameters, where a high p-value indicates no statistically significant effect in the model. The $R^2$ metric measures the proportion of variance explained. Models with a higher adjusted $R^2$ are generally preferred, as this indicates that the model explains a higher proportion of the variability within the data. Predictive accuracy is assessed through root mean squared error (RMSE). It measures the average magnitude of the residuals, with lower values indicating that the model's predictions are closer to the observed data. The AIC is used to balance model fit with complexity by penalising the number of parameters included in the model. A model with lower AIC is preferred, as it explains the data well without including unnecessary parameters.

## Model selection

Based on the exploratory analysis, the final model was selected to capture key predictors and account for non-linearities and interactions while balancing simplicity:

\begin{eqnarray*}
 M_F:  y_i = \beta_0 + \beta_1 \text{solar}_i + \beta_2 \text{wdaynumber}_i + \beta_3 \text{month}_i + \beta_4 \text{year}_i + \beta_5 \text{DSN}^2_i + \beta_6 \text{TE}_i  + \beta_7 \, \text{DSN}^2\text{:month}_i + \epsilon_i
\end{eqnarray*}
where $\epsilon_i \overset{\mathrm{iid}}\sim \mathcal{N}(0, \sigma^2)$.

The `TE` variable was selected as the temperature measure because it incorporates both the current and previous day information, capturing lagged effects. Models using `TE` produced a higher $R^2$, lower RMSE and lower AIC indicating a better overall model fit and predictive performance. The `DSN` variable exhibited a quadratic relationship with demand, so $DSN^2$ was included. `Month` and `Year` were included as factor variables to account for annual variation. A distinct dip in demand in December motivated the inclusion of an interaction between `DSN^2` and `Month`, allowing the model to capture monthly variations in the quadratic effect and substantially improving the AIC by 1,846.95. 

```{r TE, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
formulas <- list(m.temp, m.to, m.final)
names <- c("Model with temp", "Model with TO", "Model with TE")

# creating the comparison table
comparison_df <- comparison(formulas, names, processed.data)
colnames(comparison_df) <- c("Model", "$R^2$", "Adjusted $R^2$", "RMSE", "AIC")
kable(comparison_df, caption = "Table 1: Model Performance Comparison")
```

The remaining variables were included due to trends seen in exploratory analysis and as they are statistically significant, mostly with $p<0.01$.

```{r results='asis', eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.width=5}

# Convert summary coefficients to a data frame
coef_table <- summary(m.final)$coefficients

# Create a nice clean table
kable(coef_table, digits = 3, caption = "Table 2: Regression Results") 
```

## Model Evaluation

The residuals were examined to check the underlying assumptions are satisfied, ensuring the model provides a valid fit to the data.

```{r Residuals, eval=TRUE, echo=FALSE, fig.height=16, fig.width=16}

# Check Residuals
par(mfrow = c(2, 2))
plot(m.final)

```
The Residuals vs Fitted plot shows that the variance remains fairly constant, supporting the homoscedacity assumption. The Q-Q plot shows a slight deviation from normality with an S-shaped pattern suggesting slight skewness. However, given the large sample size, the central limit theorem ensures the sampling distribution remains approximately normal. The Scale-Location plot shows minimal heteroscedasticity with only a slight curve in the line. This is not concerning given the large sample size and predictive focus. Finally, the residuals vs leverage plot indicates no problematic points, as none exhibit high leverage or Cook's distance suggesting no outliers.

# Cross Validation:

## Methodology
To assess model generalizability, we used expanding window cross-validation. Since, our model includes year as a factor, which would be updated by NESO annually based on their estimated year effect, we altered our cross validation method as follows:

Firstly we fit the model on an initial training set from 1991 up to 2000. Then the year effect is removed from the trained model. Using this, we compute the 'yearless predictions' for 2001. To mimic adding back in a year effect we assumed a known year effect, using the coefficient estimated from the full dataset. This was added to the 'yearless predictions' to compute our final predictions. We then compare these with the observed data for 2001 and obtain the following metrics: 

* Squared error: $\text{SE} = (y - \hat{y}_F)^2$
* Interval score: $\text{IS}(\alpha) = U_F - L_F + \frac{2}{\alpha} (L_F - y) \mathcal{1}\{y \leq L_F\} + \frac{2}{\alpha} (y - U_F) \mathcal{1}\{y > L_F\}$ where $L_F$ and $U_F$ denote the lower and upper bound of the prediction interval with coverage probability $\alpha$.

* Dawid-Sebastiani: $\text{DS} = \frac{(y - \hat{y}_F)^2}{\sigma^2_F} + \log(\sigma^2_F)$

We then repeat this process, expanding the training set by one year and moving the test set forward one year. 

## Predictive Ability and Simplicity


```{r Cross Validation Plots, eval=TRUE, echo=FALSE, fig.height=5, fig.width=7, message=FALSE, warning=FALSE}

# Plot cross validation predictions vs actual demand 
p2 <- ggplot(data = crossvalid.final, aes(x = observed, y = predictions, colour = as.factor(test_set))) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "black") +
  labs(title = "Figure 9: Predicted vs Actual Demand",
       x = "Actual Average Demand (MW)",
       y = "Predicted Average Demand (MW)") +
  scale_colour_discrete(name = "Test Set") +
  theme(text = element_text(size = 16))
p2

```


Table 3 presents the predictive scores from the cross validation for our final model and both a basic model,


$$
M_B: y_i = \beta_0 + \beta_1 \text{solar}_i + \beta_2 \text{wdaynumber}_i + \beta_3 \text{month}_i + \beta_4 \text{year}_i + \beta_5 \text{wind}_i + \beta_6 \text{temp}_i + \epsilon_i
$$

and a more complex model, 

$$
M_C: y_i = \beta_0 + \beta_1 \text{solar}_i + \beta_2 \text{wdaynumber}_i + \beta_3 \text{month}_i + \beta_4 \text{year}_i + \beta_5 \text{DSN}^2_i + \beta_6 \text{TE}_i + \\ \beta_7 \text{TE:solar}_i + \beta_7 \text{TE:month}_i + \beta_8 \text{solar:month}_i + \beta_9 \text{wdaynumber:month}_i + \beta_9 \text{DSN}^2\text{:month}_i + \epsilon_i
$$
where $\epsilon_i \overset{\mathrm{iid}}\sim \mathcal{N}(0, \sigma^2)$.


```{r Other RMSE Table, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}

# Table with scores for final model, baseline model m1 and complex model m2 
cv.other.scores <- bind_rows(cv.scores.final, cv.scores.m1, cv.scores.m2) %>%
                            mutate(Model = c("$M_F$", "$M_{B}$", "$M_{C}$")) %>%
                            select(Model, everything())%>%
                            rename(
                              "Mean Dawid Sebastiani Score" = MDS,
                              "Mean Interval Score" = MIS
                            )

# Print the table
kable(cv.other.scores, caption = "Table 3: Predictive Scores for Final, Baseline & Complex Model")

```


Each  predictive score for the final model is lower for $M_F$ than $M_B$, suggesting higher accuracy in predictions as well as confidence associated with these predictions. However, the more complex model $M_C$ achieves only marginally better scores, suggesting that a simpler, more practical model is preferable.

To see that the final model is not overfitting, observe that the fitted RMSE based on the full dataset, is greater than the Cross Validation RMSE.

```{r, Assessing Overfitting, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}

# Comparison of RMSE for model fitted on full dataset vs cross validation 
rmse.values <- c("Fitted RMSE" = scores.final$RMSE, "Cross Validation RMSE" = cv.scores.final$RMSE)

# Convert to a data frame and transpose
RMSE.data <- as.data.frame(t(rmse.values))


# Print the table
kable(RMSE.data, caption = "Table 4: How the model performs on trained data vs new data")
```

This is unusual however could be explained due to the averaging over the 14 test sets. Also note the difference is minor relative to the scale of the data, indicating that the model performs consistently on both training and new data. 

# Limitations
Next consider limitations to our method and model: 

By assuming the known year effect from the full model, we leak information about the test year into the predictions. We still choose this method for two reasons: firstly, it still minimises the disruption to the chronological structure of the data. Secondly, expanding the training set at each step, incorporates more historical data helping the model capture long-term, recurring monthly and weekly trends more effectively.

Secondly consider the observed vs predicted values from our cross validation  in Figure 9. While the points lie scattered around the 
$y=x$ line indicating accurate predictions, a  few points lie above the line. However these occur at lower demand levels and since NESO prioritises accuracy at higher demand levels where shortfalls are more likely to occur, the model remains well-suited for its purpose. 


```{r code=readLines("code.R"), eval=FALSE, echo=TRUE}
# Do not change this code chunk
```

