---
title: "Modelling Avergae Electricity Demand in Great Britain"
author: "Ella Park, Rosie Crookes, Kieran Marguerie de Rotrou"
output:
  html_document:
    number_sections: no
  pdf_document:
    number_sections: no
header-includes:
  - \newcommand{\bm}[1]{\boldsymbol{#1}}
  - \newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
---

```{r setup, include=FALSE}
# Modify this setup code chunk to set options
# or add extra packages etc if needed.
# See the project instructions for more details
# on what code to show, and where/how.

# Set default code chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE
)

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(patchwork))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(gridExtra))

theme_set(theme_bw())
# To give the same random number sequence every time the document is knited,
# making it easier to discuss the specific numbers in the text:
set.seed(12345L)
```

```{r code=readLines("code.R"), eval=TRUE, echo=FALSE, results='hide', include=FALSE}
# Do not change this code chunk
# Load function definitions
source("code.R")
```

# Executive Summary
data <- data %>%
  mutate(
    Local_DateTime = dmy_hm(`Local Time`),
    Month = month(Local_DateTime),
    WeekdayNum = wday(Local_DateTime),
    Year = year(Local_DateTime),
    Date = as.Date(Local_DateTime),
    Hour = hour(Local_DateTime),  # Extract hour
    winter_year = ifelse(Month >= 11, Year, Year - 1),
    winter_start = as.Date(paste0(winter_year, "-11-01")),
    DSN = as.numeric(Date - winter_start)
  )

Add this at the end once we have our results 

# Method and Approach

The main aim of this report was to enable NESO to estimate demand patterns in electricity over the long-term. To do this we considered several ordinary least squares (OLS) regression model each fitted to assess the association between a set of predictor variables and average daily demand. Our predictor variables included both temporal and weather based variables. We then chose a model by comparing important scores such as ____ (which ______). We further adapted it to ensure practicality and simplicity of the model. Finally we analyse the predictive ability of our model by applying cross validation to ensure the model has strong predictive ability and to ensure its not over fitting to the data set. 

# Data and Exploratory Analysis

Prior to fitting any models we examine the dataset used in our analysis. We consider the following predictor variables for our linear model: 

- $\texttt{wind_merra}$: Average capacity factor of wind generation based on wind speeds
- $\texttt{solar_sarah}$: Average capacity factor of solar generation based on solar output
- $\texttt{temp_merra1}$: Average temperature 
- $\texttt{wdayindex}$: Day of the week
- $\texttt{month}$: Month
- $\texttt{year}$: Year
- $\texttt{DNS}$: Number of days since November
- $\texttt{TO}$:  Average temperature from 3 PM – 6 PM on each day
- $\texttt{TE}$:  Average of $\texttt{TO}$ at 6 PM and $\texttt{TE}$ at 6 PM on the previous day. The $\texttt{TE}$ for 1 November 1991 was taken to be 11.37 degrees celsius.

We next examine the data visually to highlight any general trends and detect any anomalies. We use scatter plots below to examine how each predictor correlates with demand. 

```{r Exploratory Analysis, eval=TRUE, echo=FALSE, fig.height=30, fig.width=20, warning = 'false', message = 'false', results='hide'}

# Wind vs Demand
wind_plot <- ggplot(processed.data, aes(x=wind_merra1, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[1]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Peak Demand vs Wind Capacity", x="Wind Capacity Factor", y="Peak Demand (MW)") +
  theme(text = element_text(size = 16))  

# Solar vs Demand
solar_plot <- ggplot(processed.data, aes(x=solar_sarah, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[6]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Peak Demand vs Solar Capacity", x="Solar Capacity Factor", y="Peak Demand (MW)") +
  theme(text = element_text(size = 16))

# Temp vs Demand- shows negative correlation
temp_plot <- ggplot(processed.data, aes(x=temp_merra1, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[8]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Average Demand vs Temperature", x="Temperature (°C)", y="Average Demand (MW)") +
  theme(text = element_text(size = 16))  

# TE vs Demand - shows negative correlation
te_plot <- ggplot(processed.data, aes(x=TE, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[4]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Average Demand vs TE", x="TE (°C)", y="Average Demand (MW)") +
  theme(text = element_text(size = 16))  

# TO vs Demand
to_plot <- ggplot(processed.data, aes(x=TO, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[9]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Average Demand vs TO", x="TO (°C)", y="Average Demand (MW)") +
  theme(text = element_text(size = 16))  

# dsn vs Demand - shows negative correlation
dsn_plot <- ggplot(processed.data, aes(x=DSN, y=demand_gross)) +
  geom_point(alpha=0.5) +
  geom_smooth(method="lm", formula = y ~ poly(x,2), color="red") +
  labs(title="Average Demand vs DSN", x="Days since November", y="Average Demand (MW)") +
  theme(text = element_text(size = 16))  

# Month vs demand
month_plot <- ggplot(processed.data, aes(x=factor(Month, levels = c(10, 11, 12, 1, 2)), y=demand_gross, fill=factor(Month))) +
  geom_violin(alpha=0.5) +  # Create the violin plot
  geom_boxplot(width=0.2, color="black", alpha=0.7) +  
  scale_fill_viridis_d(option="viridis") +    
  scale_x_discrete(labels=c("November", "December", "Janurary", "February", "March")) +
  labs(title="Peak Demand Distribution by Month", x="Month", y="Peak Demand (MW)") +
  theme_bw() +
  guides(fill=FALSE) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Day of week
day_plot <- ggplot(processed.data, aes(x=factor(WeekdayNum), y=demand_gross, fill=factor(WeekdayNum))) +
  geom_violin(alpha=0.5) +  # Create the violin plot
  geom_boxplot(width=0.2, color="black", alpha=0.7) +  
  scale_fill_viridis_d(option="viridis") + 
  scale_x_discrete(labels=c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")) +
  labs(title="Peak Demand by Day of the Week", x="Day of Week", y="Peak Demand (MW)") +
  theme_bw() +
  guides(fill=FALSE) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Arrange in a grid
(wind_plot + solar_plot) / (temp_plot + to_plot) / (te_plot + dsn_plot) /  (month_plot + day_plot)   
  plot_annotation(title = "Exploratory Analysis of Demand Data", theme = theme(plot.title = element_text(size = 20, face = "bold")))
```


Key observations from the analysis indicate that wind has no clear correlation with average demand and was therefore removed from the model as it provides no predictive value. Most the solar data are concentrated between 0 and 0.05, with higher solar generation associated with lower average demand, suggesting an inverse relationship. This aligns with expectations, as increased sunlight reduces the need for lighting, lowering electricity demand. Temperature, TO and TE all show a negative linear correlation with average demand, suggesting that demand tends to decrease as temperature rises. This is intuitive since as warmer conditions generally reduce heating needs. TE shows the strongest negative effect and is considered the best temperature measure because it incorporates information from both today and the previous day. Its importance lies in averaging with the past day, capturing a lagged effect that may reflect customers updating heating settings with a delay. The days since november variable follows an overall quadratic distribution, but has a pronounced dip in December which suggests that an interaction between month and DSN should be considered in the model. Month has a clear effect on demand, with the highest levels observes in January and February and the lowest in November and March. Finally, day of the week is an important predictor to include in the model as demand is lower on weekends, highest on weekdays, with Monday through Thursday showing similar mean demand and a slight decline on Fridays. Combining weekdays and weekends reduce the resolution of these important differences. Year also has an impact on demand and will be included as a factor variable. However, this will be stripped out of the model by NESO, who will add their own year effect for future years.

# Model fitting and cross-validation results

## Metrics & Linear Models

We aim to fit a ordinary least-squares linear regression model of the form $$\mathbf{y} = X \boldsymbol{\beta} + \boldsymbol{\epsilon}.$$ In this type of model we assume the following 

* Linearity of the model parameters. 
* The errors, $\boldsymbol{\epsilon}$, are normally distributed. 
* The errors are independent.
* The errors have constant variance (homoscedasticity).

### Hypothesis Testing

To check whether a variable is significant in the model or not, we use the following hypothesis test about the model parameters.
\begin{eqnarray*}
    H_0&:& \beta_i = 0 \\
    H_1&:& \beta_i \neq 0.
\end{eqnarray*}
using the test statistic $T = \frac{\hat{\beta}_i - \beta_0}{\hat{\sigma}_{\hat{\beta}_i}} \sim t_{n-p}$. The resulting p-value of the test gives the probability of observing a value as or more extreme than the observed test statistic given that $H_0$ is true. The model summary output gives the p-value of these tests for each of the model parameters. A high p-value suggests that we cannot reject the null hypothesis that $H_0$ is true, that is that the associated parameter in the model is equal to 0, and the variable has no statistically significant effect in the model.

### Adjusted R-squared

The $R^2$ metric measures the proportion of variance explained by the linear model. Formally it is defined as 
\begin{equation*}
    r^2 = 1 - \frac{\sum_{i}^{n}(y_i - \hat{\mu}_i)^2}{\sum_{i}^{n}(y_i - \overline{y})^2}
\end{equation*}
where $\mu_i$ is the estimated value of $y_i$ by the model and $\bar{y}$ is the sample mean of the data. The $R^2$ tends to overestimate how well a model is doing, and so to account for this we look at the adjusted $R^2$ metric which accounts for the number of parameters in the model.
\begin{equation*}
    r^2_{\text{adj}} = 1 - \frac{\frac{\sum_{i}^{n}(y_i - \hat{\mu}_i)^2}{(n-p)}} {\frac{\sum_{i}^{n}(y_i - \overline{y})^2}{(n-1)}}
\end{equation*}
Models with a higher adjusted $R^2$ are preferable over models with a lower one, as this indicates that the model explains a higher proportion of the variability within the data. 


### Akaike Information Criterion (AIC)

The AIC is a metric used to compare the quality of a model for a given dataset. It balances model fit with model complexity by penalising the number of parameters included in the model. Formally, AIC is defined as
\begin{equation*}
AIC=2p-2\ln(L)
\end{equation*}
where $p$ is the number of parameters in the model and $L$ is the maximised value of the likelihood function.

Lower AIC indicate a better balance between fit and model complexity. When comparing models, the model with the lowest AIC is generally preferred, as it explains the data well without including unnecessary parameters.

## Model selection

Based on the exploratory data analysis, the final model for predicting average demand was selected to capture the most important predictors and accounting for non-linearities and interactions identified in the data while balancing simplicity. The final model is:

not sure if this is the best way to format it but thought I would add it in for now...

\begin{eqnarray*}
 M_F:  y_i = \beta_0 + \beta_1 \text{solar}_i + \beta_2 \text{wdaynumber}_i + \beta_3 \text{month}_i + \beta_4 \text{year}_i + \beta_5 \text{DSN}_i + \beta_6 \text{TE}_i  + \beta_7 \, \text{DSN}^2\text{:month} + \epsilon_i
\end{eqnarray*}
where $\epsilon_i \overset{\mathrm{iid}}\sim \mathcal{N}(0, \sigma^2)$.


The `TE` variable was selected as the temperature predictor variable because it incorporates information from both the current and previous day, capturing potential lagged effects in demand. This choice was supported quantitatively. When comparing modes that differed only in the temperature variable, `TE` produced a higher $R^2$, lower RMSE and lower AIC indicating a better overall model fit and predictive performance.

```{r TE, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
formulas <- list(m.temp, m.to, m.final)
names <- c("Model with temp", "Model with TO", "Model with TE")

# creating the comparison table
comparison_df <- comparison(formulas, names, processed.data)
colnames(comparison_df) <- c("Model", "$R^2$", "Adjusted $R^2$", "RMSE", "AIC")
<<<<<<< HEAD
kable(comparison_df)
=======
kable(comparison_df, caption = "Model Performance Comparison")
>>>>>>> origin/main
```


Solar generation was included because higher solar output was associated with lower average demand and was statistically significant in the final model, $p<0.01$. Day of the week effects were modelled as a factor to account lower weekend demand and small variations across weekdays with all factor levels highly significant, $p<0.01$. The variable `DSN` (days since November) exhibited a quadratic relation with demand in the earlier analysis so $DSN^2$ was included to capture this non-linear trend. `Month` and `Year` were included as factor variables to account for seasonal and annual variation in demand. A pronounced dip in December motivated the inclusion of an interaction between `DSN^2` and `Month`, allowing the model to account for monthly variations in the quadratic effect and substantially improving the AIC (decrease of 1,846.95).

While the inclusion of all two-way interactions followed by backwards selection yielded a slightly higher adjusted $R^2$, this model was chosen to favour simplicity, as the additional interaction terms contributed only marginal effects.

```{r results='asis', eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.width=5}

# Convert summary coefficients to a data frame
coef_table <- summary(m.final)$coefficients

# Create a nice clean table
kable(coef_table, digits = 3, caption = "Table 1: Regression Results") 


```

## Evaluation of Model



```{r Exploratory Analysis 2, eval=FALSE, echo=FALSE, warning=FALSE, message=FALSE}
# Q2: HOW WELL DOES THE MODEL FIT HISTORIC DATA



# Calculate Root Mean Squared Error (baseline model)
rmse_m0 <- sqrt(mean((demand.data$demand_gross - predict(m0))^2))

# Calculate Root Mean Squared Error (our model)
rmse_finalmodel <- sqrt(mean((demand.data$demand_gross - predict(chosen.mod))^2))

# Create the data frame
table_data <- data.frame(
  Model = c("$M_0$", "$M_F$"),
  RMSE = c(rmse_m0, rmse_finalmodel),
  "$R^2$" = c(0.4636, 0.7779),
  check.names = FALSE
)

# Print the table
kable(table_data, caption = "Model Performance Comparison")

```




```{r Residuals, eval=FALSE, echo=FALSE, fig.height=16, fig.width=16}

# Check Residuals
par(mfrow = c(2, 2))
plot(chosen.mod)

```

T


```{r Actual vs Predicted Demand, eval=FALSE, echo=FALSE}

#Plot Actual vs Predicted Demand
ggplot(demand.data, aes(x=demand_gross, y=predict(chosen.mod))) +
  geom_point(alpha=0.6) +
  geom_abline(slope=1, intercept=0, color="red") +
  labs(title="Actual vs. Predicted Average Demand",
       x="Actual Demand", y="Predicted Demand") +
  theme_minimal()

```





## Cross Validation:

### Methodology
To assess model generalizability, we analysed our model using expanding window cross-validation. However, our model includes year as a factor which would be updated by NESO each year based on their subjectively estimated year effect. To mimic this process we altered our cross validation method as follows:

Firstly we fit the model on an initial training set from 1991 up to 2000. Then any the year effect is removed from the trained model. Using this trained model, we compute the 'yearless predictions' for 2001. To mimic adding back in a year effect (as NESO would) we assumed a known year effect, using the coefficient estimated from the full dataset. This was added to the yearless predictions' to compute our final predictions. We then compare these with the observed data for 2001 and obtain the following metrics: 

* Squared error: $\text{SE} = (y - \hat{y}_F)^2$
* Interval score: $\text{IS}(\alpha) = U_F - L_F + \frac{2}{\alpha} (L_F - y) \mathcal{1}\{y \leq L_F\} + \frac{2}{\alpha} (y - U_F) \mathcal{1}\{y > L_F\}$ where $L_F$ and $U_F$ denote the lower and upper bound of the prediction interval with coverage probability $\alpha$.

* Dawid-Sebastiani: $\text{DS} = \frac{(y - \hat{y}_F)^2}{\sigma^2_F} + \log(\sigma^2_F)$

We then repeat this process, expanding the training set by one year and moving the test set forward one year. 

There are drawbacks to this method: for example by assuming the known year effect from the full model, we leak information about the test year into the predictions. However we still choose this method for two reasons: Firstly, it still minimises the disrupt to the chronilogical strucutre of the data. Secondly, expanding the training set at each step, incorporates more historical data helping the model capture long-term, recurring seasonal and weekly trends more effectively.  

### Results

The observed vs predicted values from our cross validation are plotted in Figure 2. The points lie scattered around the $y=x$ line indicating accuracy in predictions. It is important to note that there are some points that stray above the $y=x$ line. However these occur at lower demand levels. Since NESO are particularly interested in accuracy at higher demand levels where shortfalls are likely to occur, this indicates that the model still remains well-suited for accurately estimating average daily demand, especially at high levels.

```{r Cross Validation Plots, eval=TRUE, echo=FALSE, fig.height=5, fig.width=7, message=FALSE, warning=FALSE}

p2 <- ggplot(data = crossvalid.final, aes(x = observed, y = predictions, colour = as.factor(test_set))) +
  
  geom_point() +
  
  geom_abline(slope = 1, intercept = 0, color = "black") +
  
  labs(title = "Figure 2: Predicted vs Actual Demand",
       
       x = "Actual Demand",
       
       y = "Predicted Demand") +
  
  scale_colour_discrete(name = "Training Set") +
  
  theme(text = element_text(size = 16))


p2

```

Using cross validation we find the following predictive scores.

```{r RMSE Table, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}


cv.scores.table <- cv.scores.final %>%
                            mutate(Model = "Final") %>%
                            select(Model, everything()) %>%
                            rename(
                              "Mean Dawid Sebastiani Score" = MDS,
                              "Mean Interval Score" = MIS
                            )


# Print the table
kable(cv.scores.table, caption = "Predictive Scores for Final Model")

```

To interpret these results its useful to compare it to a baseline model $M_1$ and a more complex model $M_2$:

```{r Other RMSE Table, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}


cv.other.scores <- bind_rows(cv.scores.m1, cv.scores.m2) %>%
                            mutate(Model = c("$M_1$", "$M_2$")) %>%
                            select(Model, everything())%>%
                            rename(
                              "Mean Dawid Sebastiani Score" = MDS,
                              "Mean Interval Score" = MIS
                            )

# Print the table
kable(cv.other.scores, caption = "Predictive Scores Comparison by Model")

```


Observe that each score for the final model is lower compared to $M_1$. This suggests higher accuracy in predictions as well as confidence associated with these predictions. On the other hand observe that the scores for the model $M_2$ with several interactions are marginally smaller than our final model. This indicates that simplifying the model the interest of practicality is justified.

To assess whether our final model is overfitting, we compare the RMSE based on the full set of data with the RMSE derived from the Cross Validation:

```{r, Assessing Overfitting, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
# Create the data frame
rmse.values <- c("Fitted RMSE" = scores.final$RMSE, "Cross Validation RMSE" = cv.scores.final$RMSE)

# Convert to a data frame and transpose
RMSE.data <- as.data.frame(t(rmse.values))


# Print the table
kable(RMSE.data, caption = "How the model performs on trained data vs new data")

```

The fitted RMSE is slightly lower than the cross-validation RMSE, likely due to smaller estimation and prediction sets in cross-validation. However, the difference is small relative to the data scale and not significant, suggesting the model does not perform significantly better on the data it was trained on compared to new data. Therefore we conclude the final model is not overfitting.





