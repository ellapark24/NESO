---
title: "Modelling Avergae Electricity Demand in Great Britain"
author: "Ella Park, Rosie Crookes, Kieran Marguerie de Rotrou"
output:
  html_document:
    number_sections: no
  pdf_document:
    number_sections: no
header-includes:
  - \newcommand{\bm}[1]{\boldsymbol{#1}}
  - \newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
---

```{r setup, include=FALSE}
# Modify this setup code chunk to set options
# or add extra packages etc if needed.
# See the project instructions for more details
# on what code to show, and where/how.

# Set default code chunk options
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE
)

suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(patchwork))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(gridExtra))

theme_set(theme_bw())
# To give the same random number sequence every time the document is knited,
# making it easier to discuss the specific numbers in the text:
set.seed(12345L)
```

```{r code=readLines("code.R"), eval=TRUE, echo=FALSE, results='hide', include=FALSE}
# Do not change this code chunk
# Load function definitions
source("code.R")
```

# Executive Summary
 

# Data and Exploratory Analysis

Prior to fitting any models we examine the dataset used in our analysis. We consider the following predictor variables for our linear model: 

- $\texttt{wind_merra}$: Average capacity factor of wind generation
- $\texttt{solar_sarah}$: Average capacity factor of solar generation
- $\texttt{temp_merra1}$: Average temperature 
- $\texttt{wdayindex}$: Day of the week
- $\texttt{month}$: Month
- $\texttt{year}$: Year
- $\texttt{DNS}$: Number of days since November
- $\texttt{TO}$:  Average temperature from 3 PM – 6 PM on each day
- $\texttt{TE}$:  Average of $\texttt{TO}$ at 6 PM and $\texttt{TE}$ at 6 PM on the previous day. The $\texttt{TE}$ for 1 November 1991 was taken to be 11.37 degrees celsius.

We visually inspected scatter plots of each predictor against demand to correlations and potential anomalies.

```{r Exploratory Analysis, eval=TRUE, echo=FALSE, fig.height=30, fig.width=20, warning = 'false', message = 'false', results='hide'}

# Wind vs Demand
wind_plot <- ggplot(processed.data, aes(x=wind_merra1, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[1]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Peak Demand vs Wind Capacity", x="Wind Capacity Factor", y="Peak Demand (MW)") +
  theme(text = element_text(size = 16))  

# Solar vs Demand
solar_plot <- ggplot(processed.data, aes(x=solar_sarah, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[6]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Peak Demand vs Solar Capacity", x="Solar Capacity Factor", y="Peak Demand (MW)") +
  theme(text = element_text(size = 16))

# Temp vs Demand- shows negative correlation
temp_plot <- ggplot(processed.data, aes(x=temp_merra1, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[8]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Average Demand vs Temperature", x="Temperature (°C)", y="Average Demand (MW)") +
  theme(text = element_text(size = 16))  

# TE vs Demand - shows negative correlation
te_plot <- ggplot(processed.data, aes(x=TE, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[4]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Average Demand vs TE", x="TE (°C)", y="Average Demand (MW)") +
  theme(text = element_text(size = 16))  

# TO vs Demand
to_plot <- ggplot(processed.data, aes(x=TO, y=demand_gross)) +
  geom_point(alpha=0.5, color=viridis(9)[9]) +
  geom_smooth(method="lm", color="red") +
  labs(title="Average Demand vs TO", x="TO (°C)", y="Average Demand (MW)") +
  theme(text = element_text(size = 16))  

# dsn vs Demand - shows negative correlation
dsn_plot <- ggplot(processed.data, aes(x=DSN, y=demand_gross)) +
  geom_point(alpha=0.5) +
  geom_smooth(method="lm", formula = y ~ poly(x,2), color="red") +
  labs(title="Average Demand vs DSN", x="Days since November", y="Average Demand (MW)") +
  theme(text = element_text(size = 16))  

# Month vs demand
month_plot <- ggplot(processed.data, aes(x=factor(Month, levels = c(10, 11, 12, 1, 2)), y=demand_gross, fill=factor(Month))) +
  geom_violin(alpha=0.5) +  # Create the violin plot
  geom_boxplot(width=0.2, color="black", alpha=0.7) +  
  scale_fill_viridis_d(option="viridis") +    
  scale_x_discrete(labels=c("November", "December", "Janurary", "February", "March")) +
  labs(title="Peak Demand Distribution by Month", x="Month", y="Peak Demand (MW)") +
  theme_bw() +
  guides(fill=FALSE) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Day of week
day_plot <- ggplot(processed.data, aes(x=factor(WeekdayNum), y=demand_gross, fill=factor(WeekdayNum))) +
  geom_violin(alpha=0.5) +  # Create the violin plot
  geom_boxplot(width=0.2, color="black", alpha=0.7) +  
  scale_fill_viridis_d(option="viridis") + 
  scale_x_discrete(labels=c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")) +
  labs(title="Peak Demand by Day of the Week", x="Day of Week", y="Peak Demand (MW)") +
  theme_bw() +
  guides(fill=FALSE) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Arrange in a grid
(wind_plot + solar_plot) / (temp_plot + to_plot) / (te_plot + dsn_plot) /  (month_plot + day_plot)   
  plot_annotation(title = "Exploratory Analysis of Demand Data", theme = theme(plot.title = element_text(size = 20, face = "bold")))
```


Wind generation showed no clear correlation with demand and was therefore excluded from further modelling as it provides no predictive value. In contrast, solar generation exhibited an inverse relationship with demand. This aligns with expectations, as increased sunlight reduces the need for lighting, lowering electricity demand. 

Temperature variables all showed a negative correlation with demand. This is intuitive since as warmer conditions generally reduce heating needs. TE shows the strongest negative effect and is considered the best temperature measure because it incorporates a lagged effect that may reflect customers updating heating settings with a delay.

The DSN variable follows a quadratic distribution, but has a pronounced dip in December. This suggests that an interaction between month and DSN should be considered in the model.

Month has a clear effect on demand, with the highest levels observes in January and lowest in November and March. Day of the week is an important predictor to include in the model as demand is lower on weekends, highest on weekdays. However, combining weekdays and weekends reduce the resolution of important ... differences. Year also has an impact on demand and will be included as a factor variable. However, this will be stripped out of the model by NESO, who will add their own year effect for future years.

# Model fitting and cross-validation results

## Metrics & Linear Models

We aim to fit a ordinary least-squares linear regression model of the form $$\mathbf{y} = X \boldsymbol{\beta} + \boldsymbol{\epsilon}$$ assuming 

* Linearity of model parameters. 
* The errors, $\boldsymbol{\epsilon}$, are normally distributed. 
* The errors are independent.
* The errors have constant variance (homoscedasticity).

### Metrics

Variable significance is assessed using hypothesis test on model parameters, where a high p-value indicates no statistically significant effect in the model. The $R^2$ metric measures the proportion of variance explained by the linear model. Models with a higher adjusted $R^2$ are preferable over models with a lower one, as this indicates that the model explains a higher proportion of the variability within the data. Predictive accuracy is assessed through root mean squared error (RMSE). It measures the average magnitude of the residuals, with lower values indicating that the model's predictions are closer to the observed data. The AIC is used to compare the quality of a model balancing model fit with complexity by penalising the number of parameters included in the model. A model with the lowest AIC is generally preferred, as it explains the data well without including unnecessary parameters.

## Model selection

Based on the exploratory data analysis, the final model was selected to capture the most important predictors and accounting for non-linearities and interactions while balancing simplicity:

\begin{eqnarray*}
 M_F:  y_i = \beta_0 + \beta_1 \text{solar}_i + \beta_2 \text{wdaynumber}_i + \beta_3 \text{month}_i + \beta_4 \text{year}_i + \beta_5 \text{DSN}^2_i + \beta_6 \text{TE}_i  + \beta_7 \, \text{DSN}^2\text{:month} + \epsilon_i
\end{eqnarray*}
where $\epsilon_i \overset{\mathrm{iid}}\sim \mathcal{N}(0, \sigma^2)$.


The `TE` variable was selected as the temperature measure because it incorporates information from both the current and previous day, capturing lagged effects in demand.  When compared  to alternatives, the model using `TE` produced a higher $R^2$, lower RMSE and lower AIC indicating a better overall model fit and predictive performance.

```{r TE, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
formulas <- list(m.temp, m.to, m.final)
names <- c("Model with temp", "Model with TO", "Model with TE")

# creating the comparison table
comparison_df <- comparison(formulas, names, processed.data)
colnames(comparison_df) <- c("Model", "$R^2$", "Adjusted $R^2$", "RMSE", "AIC")
kable(comparison_df, caption = "Model Performance Comparison")
```


Solar generation was included as higher solar output was associated with lower average demand and remained statistically significant in the final model, ($p<0.01$). Day-of-the week effects were modelled as a factor to capture the reduction weekend demand and small variations across weekdays, with all factor levels highly significant, ($p<0.01$). The `DSN` (days since November) variable exhibited a quadratic relationship with demand, so $DSN^2$ was included to capture this non-linearity. `Month` and `Year` were included as factor variables to account for seasonal and annual variation. A distinct dip in demand in December motivated the inclusion of an interaction between `DSN^2` and `Month`, allowing the model to capture monthly variations in the quadratic effect and substantially improving the AIC (decrease of 1,846.95).

```{r results='asis', eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.width=5}

# Convert summary coefficients to a data frame
coef_table <- summary(m.final)$coefficients

# Create a nice clean table
kable(coef_table, digits = 3, caption = "Table 1: Regression Results") 


```

## Evaluation of Model

To assess model performance, we  first compare the $R^2$, adjusted $R^2$, AIC and RMSE of the final model to alternatives. In particular, we compare against 

Model with no interactions: $$y_i = \beta_0 + \beta_1 \text{solar}_i + \beta_2 \text{wdaynumber}_i + \beta_3\text{month}_i + \beta_4 \text{year}_i + \beta_5 \text{DSN}^2_i + \beta_6 \text{TE}_i + \epsilon_i$$ and backward selected interaction model: $$y_i = \beta_0 + \beta_1 \text{solar}_i + \beta_2 \text{wdaynumber}_i + \beta_3\text{month}_i + \beta_4 \text{year}_i+ \beta_5 \text{DSN}^2_i + \beta_6 \text{TE}_i + \beta_7 \text{TE:solar} + \\ \beta_8 \text{TE:month} + \beta_9 \text{solar:month} + \beta_{10} \text{wdaynumber:month} + \beta_{11} \text{DSN}^2:\text{month} + \epsilon_i$$

```{r Exploratory Analysis 2, eval=FALSE, echo=FALSE, warning=FALSE, message=FALSE}

formulas <- list(m.no_interactions, m.interactions, m.final)
names <- c("Model with no interactions", "Model with all interactions", "Final Model")

# creating the comparison table
comparison_df <- comparison(formulas, names, processed.data)
colnames(comparison_df) <- c("Model", "$R^2$", "Adjusted $R^2$", "RMSE", "AIC")
kable(comparison_df, caption = "Model Performance Comparison")

```

Although the model including all two-way interactions achieved the highest $R^2$, adjusted $R^2$ and lowest RMSE, the improvement over the final model was marginal. In contrast, the model without interactions performed worse across all metrics, with a substantially higher RMSE and poorer AIC. This indicates that the inclusion of the single interaction term between `DSN^2` and `Month` captures the dominant interaction effect without introducing unnecessary complexity. Therefore, the selected model offers a balance between predictive performance and interpretability, retaining nearly all the benefit of the more complex model while remaining more practical to use and explain.

To further assess model performance, we examine the residuals to check whether the underlying assumptions are satisfied, ensuring the model provides a valid fit to the data.

```{r Residuals, eval=TRUE, echo=FALSE, fig.height=16, fig.width=16}

# Check Residuals
par(mfrow = c(2, 2))
plot(m.final)

```
The Residuals vs Fitted plot shows that the variance remains fairly constant, supporting the homoscedacity assumption.. The Q-Q plot shows a slight deviation from normality with an S-shaped pattern suggesting slight skewness. However, given the large sample size, the central limit theorem ensures that the sampling distribution remains approximately normal. The Scale-Location plot shows minimal heteroscedasticity with only a slight curve in the red line. However this isn't too concerning given the larger sample size and focus on prediction rather than inference. Finally, the residuals vs leverage plot indicates no problematic points, as none exhibit high leverage or Cook's distance suggesting no influential outliers.


```{r Actual vs Predicted Demand, eval=FALSE, echo=FALSE}

#Plot Actual vs Predicted Demand
ggplot(processed.data, aes(x=demand_gross, y=predict(m.final))) +
  geom_point(alpha=0.6) +
  geom_abline(slope=1, intercept=0, color="red") +
  labs(title="Actual vs. Predicted Average Demand",
       x="Actual Demand", y="Predicted Demand") +
  theme_minimal()

```





## Cross Validation:

### Methodology
To assess model generalizability, we used expanding window cross-validation. Since, our model includes year as a factor, which would be updated by NESO annually based on their estimated year effect, we altered our cross validation method as follows:

Firstly we fit the model on an initial training set from 1991 up to 2000. Then any the year effect is removed from the trained model. Using this trained model, we compute the 'yearless predictions' for 2001. To mimic adding back in a year effect (as NESO would) we assumed a known year effect, using the coefficient estimated from the full dataset. This was added to the yearless predictions' to compute our final predictions. We then compare these with the observed data for 2001 and obtain the following metrics: 

* Squared error: $\text{SE} = (y - \hat{y}_F)^2$
* Interval score: $\text{IS}(\alpha) = U_F - L_F + \frac{2}{\alpha} (L_F - y) \mathcal{1}\{y \leq L_F\} + \frac{2}{\alpha} (y - U_F) \mathcal{1}\{y > L_F\}$ where $L_F$ and $U_F$ denote the lower and upper bound of the prediction interval with coverage probability $\alpha$.

* Dawid-Sebastiani: $\text{DS} = \frac{(y - \hat{y}_F)^2}{\sigma^2_F} + \log(\sigma^2_F)$

We then repeat this process, expanding the training set by one year and moving the test set forward one year. 

There are drawbacks to this method: for example by assuming the known year effect from the full model, we leak information about the test year into the predictions. However we still choose this method for two reasons: Firstly, it still minimises the disrupt to the chronilogical strucutre of the data. Secondly, expanding the training set at each step, incorporates more historical data helping the model capture long-term, recurring seasonal and weekly trends more effectively.  

### Results

The observed vs predicted values from our cross validation are plotted in Figure 2. The points lie scattered around the $y=x$ line indicating accurate predictions. A few points lie above the line, however these occur at lower demand levels. Since NESO prioritises accuracy at higher demand levels where shortfalls are more likely to occur, the model remains well-suited for estimating average daily demand, particularly during periods of high demand.

```{r Cross Validation Plots, eval=TRUE, echo=FALSE, fig.height=5, fig.width=7, message=FALSE, warning=FALSE}

p2 <- ggplot(data = crossvalid.final, aes(x = observed, y = predictions, colour = as.factor(test_set))) +
  
  geom_point() +
  
  geom_abline(slope = 1, intercept = 0, color = "black") +
  
  labs(title = "Figure 2: Predicted vs Actual Demand",
       
       x = "Actual Demand",
       
       y = "Predicted Demand") +
  
  scale_colour_discrete(name = "Training Set") +
  
  theme(text = element_text(size = 16))


p2

```

Using cross validation we find the following predictive scores.

```{r RMSE Table, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}


cv.scores.table <- cv.scores.final %>%
                            mutate(Model = "Final") %>%
                            select(Model, everything()) %>%
                            rename(
                              "Mean Dawid Sebastiani Score" = MDS,
                              "Mean Interval Score" = MIS
                            )


# Print the table
kable(cv.scores.table, caption = "Predictive Scores for Final Model")

```

To interpret these results its useful to compare it to a baseline model $M_1$ and a more complex model $M_2$:

```{r Other RMSE Table, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}


cv.other.scores <- bind_rows(cv.scores.m1, cv.scores.m2) %>%
                            mutate(Model = c("$M_1$", "$M_2$")) %>%
                            select(Model, everything())%>%
                            rename(
                              "Mean Dawid Sebastiani Score" = MDS,
                              "Mean Interval Score" = MIS
                            )

# Print the table
kable(cv.other.scores, caption = "Predictive Scores Comparison by Model")

```


Each  predictive score for the final model is lower than $M_1$, suggesting higher accuracy in predictions as well as confidence associated with these predictions. While the more complex model $M_2$ with several interactions has slightly lower scores than the final mode, the difference is marginal, supporting the choice of a simpler, more practical model.

To evaluate potential overfitting, we compare the RMSE based on the full set of data with the RMSE from the Cross Validation:

```{r, Assessing Overfitting, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
# Create the data frame
rmse.values <- c("Fitted RMSE" = scores.final$RMSE, "Cross Validation RMSE" = cv.scores.final$RMSE)

# Convert to a data frame and transpose
RMSE.data <- as.data.frame(t(rmse.values))


# Print the table
kable(RMSE.data, caption = "How the model performs on trained data vs new data")

```

The fitted RMSE is slightly lower than the cross-validation RMSE, likely reflecting the smaller estimation and prediction sets in cross-validation. However, the difference is minor relative to the scale of the data, indicating that the model performs consistently on both training and new data. This suggests that the final model is not overfitting.




